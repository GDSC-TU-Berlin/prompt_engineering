{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Engineering Workshop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb8e695006c4ea5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installieren der notwendigen Bibliotheken"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336fb8e996e4c115"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone \"https://github.com/GDSC-TU-Berlin/prompt_engineering.git\"\n",
    "!mkdir stupo\n",
    "!mv prompt_engineering/stupo/embeddings.npz stupo/embeddings.npz"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e75628859a8013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install openai numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import prompt_engineering.tests as tests\n",
    "import prompt_engineering.utils as utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3275530d3566de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ddeca092e7e8f4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hallo OpenAI\n",
    "In unserem ersten Beispiel wollen wir uns anschauen, wie wir OpenAI nutzen können um einen Text zu vervollständigen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4531a7a11bfb802"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = None\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8bab308086e42e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wie wir sehen können gibt uns OpenAI eine Antwort zurück. Diese enthält insbesondere den vervollständigten Prompt. Wir können uns auch nur den vervollständigten Prompt ausgeben lassen mit:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881dd8734a95f9bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response.choices[0].text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7686be2245d4c9a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hauptstädte vervollständigen\n",
    "In unserer ersten Aufgabe wollen wir eine Funktion schreiben, die uns die Hauptstadt eines Landes zurückgibt.\n",
    "Dafür wollen wir eine Methode get_capital(country) nutzen, welche als Eingabe eine Text country bekommt und als Ausgabe NUR die Hauptstadt zurückgibt.\n",
    "\n",
    "Zum Beispiel solle der Aufruf von get_capital(\"Deutschland\") als Antwort \"Berlin\" liefern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f113497fecf3b462"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = client.completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\"\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e58d4f203d2e86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_capital(\"Deutschland\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b769f64159eda2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eed5178068c420c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wenn wir einfach nur die Frage stellen wie wir es in ChatGPT tun würden, bekommen wir leider nicht nur den Namen der Hauptstadt zurück...\n",
    "\n",
    "Zum Beispiel liefert uns der Aufruf von get_capital(\"Deutschland\") als Antwort \"Die Hauptstadt von Deutschland ist Berlin.\" zurück. Wir wollen aber nur \"Berlin\" zurückbekommen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb17d2f846abec2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = client.completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\"\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f032ace82adbc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_capital(\"Deutschland\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3faaeaf23abef3b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7555090f9396a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verbessern der Antwort durch Beispiele\n",
    "LLMs sind sehr gut darin, Mustern zu erkennen und zu folgen. Wir können dies nutzen, um die Qualität der Antworten zu verbessern. Dafür können wir Beispiele nutzen, die wir dem LLM geben. Diese Beispiele werden dann in die Berechnung der Antwort mit einbezogen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d97fa4905131a9ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = client.completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\"\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c796d4078794bd84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1df9b78c6db08b4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chatmodels\n",
    "Wir wollen uns nun anschauen, wie wir das Problem des Hauptstädte findens mit Hilfe eines Chatmodels lösen können."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3fe073ec7488a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = None\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756ded0cd385c02e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5499535d937efeba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vokabeln lernen\n",
    "Wir wollen nun eine etwas sinnvolleren Use-Case betrachten.\n",
    "\n",
    "Angenommen wir entwickeln eine App zum lern von English Vokabeln. Hierfür wollen wir eine Methode schreiben, die ein Deutsches Wort als Eingabe bekommt und folgende Ausgaben liefert:\n",
    "- Die englische Übersetzung des Wortes\n",
    "- Die Definition des Wortes\n",
    "- Ein Beispiel für die Verwendung des Wortes in einem Satz\n",
    "- Die Wortart des Wortes\n",
    "\n",
    "Dafür wollen wir eine Methode get_english_translation entwickeln welche uns die Information als JSON zurückgibt.\n",
    " Ein Beispiel für die Ausgabe ist:\n",
    " ```\n",
    "    {\n",
    "        \"German\": \"Haus\",\n",
    "        \"English_Translation\": \"House\",\n",
    "        \"Definition\": \"A building for human habitation, especially one that consists of a ground floor and one or more upper storeys.\",\n",
    "        \"Example_Sentence\": \"The family lives in a beautiful house with a big garden.\",\n",
    "        \"Part_of_Speech\": \"Noun\"\n",
    "    }\n",
    " ```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a23abad83686ae7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_output = (\"{\"\n",
    "                  \"    \\\"German\\\": \\\"Haus\\\",\"\n",
    "                  \"    \\\"English_Translation\\\": \\\"House\\\",\"\n",
    "                  \"    \\\"Definition\\\": \\\"A building for human habitation, especially one that consists of a ground floor and one or more upper storeys.\\\",\"\n",
    "                  \"    \\\"Example_Sentence\\\": \\\"The family lives in a beautiful house with a big garden.\\\",\"\n",
    "                  \"    \\\"Part_of_Speech\\\": \\\"Noun\\\"\"\n",
    "                  \"}\")\n",
    "\n",
    "\n",
    "def get_english_translation(word):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content.strip())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ab1fe9d0d288d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_english_translation(\"Uhr\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aae8bb52bb7c69ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chatmodel mit eigenen Informationen\n",
    "Sehr häufiges werden LLM einsetzten um Chatmodele zu realisieren die Antworten geben, basierend auf eigenen Informationen. Zum Beispiel im Unternehmenskontext, um Kundenanfragen zu beantworten.\n",
    "\n",
    "Wir wollen uns nun anschauen, wie wir ein Chatbot erstellen können, der eigenes Wissen nutzt. Als Beispiel dafür wollen wir einen Chatbot erstellen, der in der Lage ist Fragen zur Informatik StuPO der TU Berlin zu beantworten."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52da6eccf2c3ef1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    context = utils.get_stupo_info(question, client)\n",
    "    info = \"\\n\".join(context)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c12ed3fdca675673"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"Wie viele Semester hat das Studium?\"\n",
    "print(answer_question(question))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878ab6cd1ef7bd84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kreatives Schreiben\n",
    "In dieser Aufgabe wollen wir uns Anschauen wie die Temperatur die Kreativität und Qualität der Antworten beeinflussen.\n",
    "\n",
    "Dafür wollen wir eine Methode geschichten_schreiber schreiben, welche als Eingabe ein Thema bekommt und als Ausgabe eine kurze Geschichte über das Thema zurückgibt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b266c149771aeeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def geschichten_schreiber(topic):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Schreibe mir eine kurze Geschichte über {topic}.\"\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ef6e41cfc05f996"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(geschichten_schreiber(\"einen Hund\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "583fd8afb7b30384"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Knobel Aufgaben mit Chain of Thoughts\n",
    "Eine sehr häufige Anwendung von LLM ist das Lösen von Knobel Aufgaben. Wir wollen uns nun anschauen, wie wir Knobel Aufgaben mit Hilfe von Chain of Thoughts lösen können und ob wir damit bessere Ergebnisse erzielen können.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "747e88fe608cc365"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_knobel_aufgabe(aufgabe):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{aufgabe}\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "681b3344a3638a41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aufgabe = \"Du schaust auf ein Portrait und ich sage Dir: Der Vater der Person auf dem Potrait ist der Sohn meines Vaters, aber ich habe keine Geschwister. Wessen Bild schaust Du an?\"\n",
    "\n",
    "print(solve_knobel_aufgabe(aufgabe))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb3a9a8cc1f2e69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_knobel_aufgaben(solve_knobel_aufgabe, client, rep=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d8c04bf66a83766"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Systemnachrichten\n",
    "Neben den Usernachrichten und Assistennachichten können wir auch Systemnachrichten nutzen. Diese dienen dazu dem System zu erklären, wie es sich verhalten soll.\n",
    "\n",
    "Wir wollen damit einen Chatbot erstellen, der immer unhöflich antwortet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd72f0215d1aff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bad_gpt(question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6b29329fae36508"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(bad_gpt(\"Was ist die Hauptstadt von Deutschland?\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b626074b704ff5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Knobel Aufgaben mit Chain of Thoughts und Self Consistency\n",
    "Wir haben uns bereits angeschaut, wie wir Knobel Aufgaben mit Chain of Thoughts lösen können. Wir haben dabei aber festgestellt das selbst bei GPT-4 die Antworten nicht immer deterministisch sind und es teilweise falsche Antworten gibt. \n",
    "\n",
    "Ein Trick um dies zu verhindern ist es, die Antworten mit Self Consistency zu kombinieren. Dabei wird die Antwort mehrfach generiert und die Antwort die am häufigsten vorkommt wird als Antwort genommen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb77621bb1bcd3a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_knobel_aufgabe_advances(aufgabe):\n",
    "    responses = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{aufgabe} Denken wir Schritt für Schritt:\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f06cb80bb128a9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "74a1812d2fd45998"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
