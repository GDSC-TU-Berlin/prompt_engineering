{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Engineering Workshop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb8e695006c4ea5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installieren der notwendigen Bibliotheken"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336fb8e996e4c115"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https: // github.com/GDSC-TU-Berlin/prompt_engineering.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8e75628859a8013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import tests\n",
    "import utils"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3275530d3566de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "openai.api_key = \"sk-mT4vZuF64TyMmCezF8XNT3BlbkFJVyPnDQCfdJXJUDvXXXXX\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ddeca092e7e8f4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hallo OpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4531a7a11bfb802"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=\"Hallo OpenAI. Wie geht es dir?\\n\\n\",  # Hier kommt der Prompt rein, der vervollständigt werden soll\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8bab308086e42e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wie wir sehen können gibt uns OpenAI eine Antwort zurück. Diese enthält insbesondere den vervollständigten Prompt. Wir können uns auch nur den vervollständigten Prompt ausgeben lassen:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881dd8734a95f9bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(response.choices[0].text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7686be2245d4c9a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hauptstädte vervollständigen\n",
    "In unserer ersten Aufgabe wollen wir eine Funktion schreiben, die uns die Hauptstadt eines Landes zurückgibt.\n",
    "Dafür wollen wir eine Methode get_capital(country) nutzen, welche als Eingabe eine Text country bekommt und als Ausgabe NUR die Hauptstadt zurückgibt.\n",
    "\n",
    "Zum Beispiel solle der Aufruf von get_capital(\"Deutschland\") als Antwort \"Berlin\" liefern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f113497fecf3b462"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=f\"Was ist die Hauptstadt von \" + country + \"?\"\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e58d4f203d2e86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_capital(\"Deutschland\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6b769f64159eda2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6eed5178068c420c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=f\"Die Hauptstadt von {country} ist \",\n",
    "        stop=[\".\"]\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f032ace82adbc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(get_capital(\"Deutschland\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3faaeaf23abef3b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7555090f9396a16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verbessern der Antwort durch Beispiele"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d97fa4905131a9ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=f\"Die Hauptstadt von Deutschland ist Berlin.\\nDie Hauptstadt von {country} ist \",\n",
    "        stop=[\".\"],\n",
    "    )\n",
    "    return response.choices[0].text.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c796d4078794bd84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1df9b78c6db08b4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chatmodels\n",
    "Wir wollen uns nun anschauen, wie wir das Problem des Hauptstädte findens mit hilfe eines Chatmodels lösen können."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a3fe073ec7488a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_capital(country):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Die Hauptstadt von Deutschland ist \"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Berlin.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Die Hauptstadt von \" + country + \" ist \"\n",
    "            }\n",
    "        ],\n",
    "        stop=[\".\"],\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "756ded0cd385c02e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_get_capital(get_capital)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5499535d937efeba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Vokabeln lernen\n",
    "Wir wollen nun eine etwas sinnvolleren Use-Case betrachten.\n",
    "\n",
    "Angenommen wir entwickeln eine App zum lern von English Vokabeln. Hierfür wollen wir eine Methode schreiben, die ein Deutsches Wort als Eingabe bekommt und folgende Ausgaben liefert:\n",
    "- Die englische Übersetzung des Wortes\n",
    "- Die Definition des Wortes\n",
    "- Ein Beispiel für die Verwendung des Wortes in einem Satz\n",
    "- Die Wortart des Wortes\n",
    "\n",
    "Dafür wollen wir eine Methode get_english_translation entwickeln welche uns die Information als JSON zurückgibt.\n",
    " Ein Beispiel für die Ausgabe ist:\n",
    " ```\n",
    "    {\n",
    "        \"German\": \"Haus\",\n",
    "        \"English_Translation\": \"House\",\n",
    "        \"Definition\": \"A building for human habitation, especially one that consists of a ground floor and one or more upper storeys.\",\n",
    "        \"Example_Sentence\": \"The family lives in a beautiful house with a big garden.\",\n",
    "        \"Part_of_Speech\": \"Noun\"\n",
    "    }\n",
    " ```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a23abad83686ae7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_output = (\"{\"\n",
    "                  \"    \\\"German\\\": \\\"Haus\\\",\"\n",
    "                  \"    \\\"English_Translation\\\": \\\"House\\\",\"\n",
    "                  \"    \\\"Definition\\\": \\\"A building for human habitation, especially one that consists of a ground floor and one or more upper storeys.\\\",\"\n",
    "                  \"    \\\"Example_Sentence\\\": \\\"The family lives in a beautiful house with a big garden.\\\",\"\n",
    "                  \"    \\\"Part_of_Speech\\\": \\\"Noun\\\"\"\n",
    "                  \"}\")\n",
    "\n",
    "\n",
    "def get_english_translation(word):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Haus\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{example_output}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{word}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content.strip())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ab1fe9d0d288d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_english_translation(\"Uhr\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aae8bb52bb7c69ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Chatmodel mit eigenen Informationen\n",
    "Sehr häufiges werden LLM einsetzten um Chatmodele zu realisieren die Antworten geben, basierend auf eigenen Informationen. Zum Beispiel im Unternehmenskontext, um Kundenanfragen zu beantworten.\n",
    "\n",
    "Wir wollen uns nun anschauen, wie wir ein Chatbot erstellen können, der eigenes Wissen nutzt. Als Beispiel dafür wollen wir einen Chatbot erstellen, der in der Lage ist Fragen zur Informatik StuPO der TU Berlin zu beantworten."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52da6eccf2c3ef1d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    context = utils.get_stupo_info(question)\n",
    "    info = \"\\n\".join(context)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Ich habe die folgende Frage: {question}. Diese Information habe ich dazu gefunden: {info}\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c12ed3fdca675673"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"Was passiert bei einem Betrugsversuch?\"\n",
    "print(answer_question(question))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "878ab6cd1ef7bd84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Knobel Aufgaben mit Chain of Thoughts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "747e88fe608cc365"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_knobel_aufgabe(aufgabe):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{aufgabe} Denken wir Schritt für Schritt.\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "681b3344a3638a41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aufgabe = \"Du schaust auf ein Portrait und ich sage Dir: Der Vater der Person auf dem Potrait ist der Sohn meines Vaters, aber ich habe keine Geschwister. Wessen Bild schaust Du an?\"\n",
    "\n",
    "print(solve_knobel_aufgabe(aufgabe))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdb3a9a8cc1f2e69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tests.test_knobel_aufgaben(solve_knobel_aufgabe, rep=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d8c04bf66a83766"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Systemnachrichten\n",
    "Neben den Usernachrichten und Assistennachichten können wir auch Systemnachrichten nutzen. Diese dienen dazu dem System zu erklären, wie es sich verhalten soll.\n",
    "\n",
    "Wir wollen damit einen Chatbot erstellen, der immer unhöflich antwortet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd72f0215d1aff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bad_gpt(question):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"Du bist ein Chatbot der immer leicht genervt Antworten gibt.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6b29329fae36508"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(bad_gpt(\"Was ist die Hauptstadt von Deutschland?\"))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72b626074b704ff5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Knobel Aufgaben mit Chain of Thoughts und Self Consistency\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb77621bb1bcd3a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def solve_knobel_aufgabe_advances(aufgabe):\n",
    "    responses = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{aufgabe} Denken wir Schritt für Schritt:\"\n",
    "            },\n",
    "        ],\n",
    "        n=3,\n",
    "    )\n",
    "    answers = \"\"\n",
    "    for i, response in enumerate(responses.choices):\n",
    "        answers += \"\\n\" + f\"Antwort {i}: \" + response.message.content.strip()\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"Du hast meherer Anworten auf eine Logik frage bekommen. Gib die Antwort an, die am häufigsten vorkommt.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Lösungen: {answers}\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94f06cb80bb128a9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "74a1812d2fd45998"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
